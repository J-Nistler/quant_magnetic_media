---
title: 
    "A Quantitative Approach to the Status of the Magnetic Media Crisis: 
    From Hoping to Knowing"
author:
    - "Jared Nistler"
    - "Sarah Nguyen"
    - "Darach Miller"
tags: 
    - archives management
    - digitization
    - preservation
    - magnetic media
    - audio/visual media
    - quantitative research
abstract: |
    This is a quantitative study on the status of magnetic media collections.
    Data collected from forty-nine archive institutions, across the United 
    States and Canada, reveals the degree institutions are at risk in losing 
    historical and cultural magnetic records due to the lack of bandwidth in 
    digitizing collections, substandard environmental conditions, and the rapid
    deterioration rate of magnetic carriers. From a ten-question survey, data 
    reflects archivists managing magnetic media collections are not confident
    in their assessments of the once-ubiquitous materials. Numbers reveal 
    cognitive dissonance from clear discrepancies between quantities of format
    type compared to archivists’ self-reflection on preservation practice and 
    estimated digitization rates before obsolescence. The majority expressed 
    confidence in digitizing 75% or more of their collection, but calculating 
    that against actual digitization rates left more magnetic items 
    undigitized. The data also displays visible trend lines between carrier 
    type risk-level and the institutions holding similar quantities of each 
    format type. These patterns have the potential to assist with the challenge 
    of the medium’s limited lifespan as digitization and training hubs can be 
    concentrated near those institutions with similar format types. In a race against    
    the clock, archives are limited by funds and time as they attempt to preserve 
    valuable media before it degrades into unreadability. Data from this study
    will offer numbers to support institutions and archivists to obtain funding
    and prepare for the digitization to preservation processes before twenty 
    years pass and the magnetic media content is just a sticky memory. 
output:
    html_document:
        toc: true
        theme: journal
---

```{r set_global_knit_options,echo=F,message=F,error=T}
# This is to just globally mute a lot of messages and errors
knitr::opts_chunk$set(echo=F,message=F,error=T,warning=F)
```

```{r setting_figures_directory}
fig_dir <- "./figures/"
```


<!-- this is commented out
    Do not start things with tabs or 4+ spaces
    Markdown interprets that as code
    -->

# Introduction

During the twentieth century, vast amounts of valuable historical and cultural
information were recorded on magnetic tape—a recording medium made of “a thin
layer capable of recording a magnetic signal supported by a thicker film
backing.” These items, known as “magnetic media,” include objects such as VHS,
compact cassette, U-matic, 7-inch reels, and more. While cost-effective and
accessible at the time, magnetic media has a limited archival lifespan. At the
end of its life, the magnetic coating can begin to degrade and decay,
especially when stored in suboptimal conditions, endangering the survival of
its contents. Additionally, as these formats age, the availability of playback
equipment and experts who can repair and maintain that equipment decrease with
every year until they are completely obsolete. In a race against the clock,
archival institutions are limited both by funds and time as they attempt to
preserve valuable media before it degrades into unreadability. These
institutions must carefully schedule and prioritize their digitization efforts,
taking into account their mission, the contents of their collections, and the
type of material which they accession.

It is widely recognized that archivists are facing a “magnetic media crisis,”
but unfortunately, any existing quantitative literature on the issue is sorely
lacking. Few attempts have been made to systematically assess the state of
audiovisual (A/V) magnetic media collections. That quantitative gap is the
focus of this project.

This study surveyed academic libraries and archives in order to gather
quantitative data on collection size, format composition, and digitization
percentage to evaluate the current state of magnetic media collections. Several
A/V archivists currently working in the field were also interviewed to gather a
qualitative understanding of their collection management strategies and
digitization workflows. Our results will illustrate the scope of the issue,
helping archival institutions to better advocate for their preservation needs.

# Background: What is Magnetic Media?

Magnetic tape is a long, thin strip of material composed of several polymer
layers. The “top coat” of the polymer contains magnetic pigments that encode
audio or visual information, and it adheres to a “substrate” layer which
supports the thin top coat (see Figure 1). On many tapes, especially those
produced after the 1970s, a third layer of backing is attached to the other
side of the substrate in order to reduce friction and static. For recording,
playback and storage purposes, the tape is generally wound around reels or
rolled up inside specialized plastic cassettes. Into the early twenty-first
century, magnetic tape was used by television broadcasters, artists and the
entertainment industry, government and nonprofit organizations, educators, and
countless private individuals; all of whom recorded nigh-endless amounts of
cultural heritage. 

Unfortunately, magnetic tape has several inherent vices that have led to a
preservation crisis. Mike Casey calls the magnetic media crisis a “perfect
storm,” because the collections are growing in size and importance (through
acquisitions, donations, and requests) as they are simultaneously fading into
obsolescence. The problems that Casey identifies for archival institutions
regarding magnetic media include “large numbers, obsolescence, degradation,
high research value, and short time window.”  The first challenge archivists
face is identifying just how much of the once-ubiquitous material exists. The
next challenge is its limited lifespan. As magnetic media ages, the top coat
and backing absorb moisture, causing the tape to become sticky and fall apart.
This condition is known as “sticky-shed syndrome” and can render the item
unplayable, as well as cause damage to playback equipment.

Limited lifespan and large numbers make the magnetic media crisis daunting
enough. However, not only are the old tapes degrading beyond readability, but
the machines, hardware, and software needed to digitize valuable content are
harder to come by and face obsolescence themselves. Furthermore, as the tools
used to work with obsolete magnetic media slowly fade into memory so do the
skills and institutional knowledge needed to operate legacy platforms.

In some cases, the content on magnetic tapes represents the clearest picture of
an event, person, or place in history, capturing speech patterns, rhythms, and
even coughs as they sputtered out in real time. That information’s potential
loss poses a serious threat to the historical record, if not addressed
immediately. 

# Literature Review: Admitting There’s a Problem

The literature on magnetic media preservation is extensive but suffers from
significant gaps—most notably a lack of quantitative data about the scope and
scale of the preservation crisis. It is widely known that magnetic tape
degrades over time and that digitizing the information stored on tape, in
preparation to preserve, is difficult and expensive. As a result, several
institutions have begun to develop and publish preservation plans. However,
there appears to be no general survey of how much magnetic media exists in
academic libraries and what state the collections are in. Not knowing the scale
of magnetic media collections that exist in libraries today makes it difficult
to present a clear picture of the crisis to administrators who ultimately
determine resource allocation. Therefore, the researcher team sought to assess
the amount of magnetic media in academic library collections, the formats in
which it exists, and the ratio of each collection that has been digitized
versus the amount left to preserve.

Magnetic tape was a significant medium of information exchange for decades, and
as such, is widely studied by archivists and industrial chemists alike.
Detailed guides to identifying and preserving different magnetic tape formats
are available for archivists accessioning new magnetic media. Libraries and
government-backed groups have published extensive surveys of tape’s chemical
properties and the potential negative reactions that it can undergo in response
to age or poor storage conditions. The physical characteristics of magnetic
tape are well-established and inform the context of our research.

The literature also clearly speaks to the difficulties and high costs of
digitization and preservation. For example, archivists at Indiana University
Bloomington’s Media Digitization and Preservation Initiative (MDPI) estimates
that their video collections will amount to 39PB of data that will need to be
stored and maintained for years to come. That is almost two hundred thousand
2TB external hard drives; not to mention the labor, infrastructure to store the
servers, preserved magnetic media, backups, and software tools associated with
the entire digitization and preservation workflow. Basic magnetic media
preservation requires, but is not limited to, these tasks listed in the
Association of Moving Image Archivists (AMIA) Videotape Preservation Fact
Sheets: “controlled temperature and humidity environmental conditions,
contamination and demagnetization prevention, tape inspection, tape cleaning,
equipment maintenance, rejuvenating unplayable tapes, staff training, and then
the entire re-formatting and conversion to digital process.” Within each
top-level task, there are specialized tools, nearly-obsolete playback hardware,
software, and meticulous techniques to conserve magnetic artifacts while
digitizing the content to the “most faithful reproduction of an original work.“

There are a variety of issues that result from magnetic material deterioration,
one being sticky-shed syndrome (SSS), and few affordable and efficient
troubleshooting techniques to address them. Many of these issues are barely
irreversible. In the case of SSS, moisture and humidity break down the tape’s
polymers that bind the magnetic particles, “causing the tape to become sticky
or gummy”, leading to the tape shedding its magnetic coating and “results in
loss of signal and therefore content.”  Alternative troubleshooting techniques
have been proposed to address SSS without the need for losing the integrity of
the magnetic tape and accelerating the degradation process, which is another
issue from the most common baking technique when attempting to preserve signal
and data loss from magnetic degradation. A 2013 article by Charles Richardson
claims to have found a miracle cure for the blight; however, there are
inconsistencies with his work. First, his paper does not provide enough detail
to properly reproduce the scientific study used to generate his data. He claims
that archivists should not use heat or chemicals on magnetic tape, then
describes a process that uses chemicals on tape. He asserts that the process
does not damage the tape, but removing the back layer and cleaning the oxide
layer may prove extremely destructive to delicate tapes. Finally, he states
that baking tapes is incredibly destructive; however, it is an industry
standard that has been working for years. Contradictory studies like these can
muddy the waters and make it difficult to create an effective digitization and
preservation plan.

With magnetic media at such a high risk for obsolescence and degradation, the
first step in facing the crisis is to establish comprehensive preservation
management solutions. Ross Harvey and Martha R. Mahard have attempted to
establish best practices in their book The Preservation Management Handbook.
According to Harvey and Mahard, the preservation management process is broken
down into five steps. First, the archivist should conduct a survey of the
facility that the collections are stored in to ensure proper temperature,
humidity, and maintenance. Second, the archivist should ensure that the
facility is prepared for any natural disasters that may occur in the local
area. Third, the archivist should evaluate the collection using available
collection management software such as UC Berkeley’s CALIPR tool or Columbia
University’s AVDb software. Fourth, the archivist should create a plan of
action based on the assessment and available resources. Finally, the archivist
should frame that plan to mesh with higher echelon institutional values and
mission statements. While this is a broad overview of managing the preservation
process, it is easier said than done.

While Harvey and Marhard’s book appears to be comprehensive, the text lacks
depth and actual guidance on how to manage a collection in practice.
Furthermore, pages dedicated to each item format lack any information on what
or how to prioritize items for preservation; arguably one of the most important
facets of archival collection management. Even though the paper is from 2014,
the recommended tools are already outdated and utilize prioritization
algorithms that are tailored to the institutions that created them. Most
importantly, the tools are not relevant in taking on new knowledge about the
degradation and obsolescence of magnetic media into account. For example,
Columbia University’s AVDb software utilizes an algorithm which was based
partly on Wheeler’s handbook and considers the majority of magnetic media to be
as stable as Blu-ray discs. There is a huge flaw in the belief that magnetic
media and even discs are stable preservation mediums—which is a completely
separate research study—but as Jason Scott of the Internet Archive has
repeatedly stated: disk storage is not more stable than tape. Each of these
mediums requires a machine and energy to run in order to retrieve and process.
All of these existing solutions have questionable dependencies to storage
mediums that are increasingly becoming obsolete each passing year.

In 2011, Indiana University Bloomington saw the failures in established
preservation management paradigms and decided to become a national leader in
creating a new standard. While managing preservation for their own institution,
Indiana University Bloomington decided to write their Meeting the Challenge of
Media Preservation report in such a way that any institution could implement
good preservation management strategies modeled off of their example; this
report includes digitization practices. Indiana University Bloomington's
emphasis is largely based around film preservation, but the strategies and
solutions provided in their report were purposely written to be inclusive to
all items in an institution’s collection. This comprehensive report outlines
all of Harvey and Mahard’s strategies, as they referenced the Indiana
University Bloomington report in their literature, but focuses on the
application rather than the theory. Any institution looking to properly address
the magnetic media crisis could look to Indiana University Bloomington as a
national leader and use what they have successfully implemented as a guide to
their own institution-specific preservation and digitization management
solutions. 

The report describes how Indiana University Bloomington reached out to partners
both on and off campus. Taking into consideration all stakeholders’ rights and
responsibilities to preservation is also recommended as best practice by Mary
Feeney out of the University of Arizona. Additionally, the report describes a
workflow where minimal metadata is recorded during the digitization process.
Larisa K. Miller at the Hoover Institution Archives describes a similar
strategy while digitizing manuscripts, separating the metadata workflow from
the digitization workflow to increase efficiency. Miller later described how
digitizing with limited metadata could ameliorate the magnetic media crisis.
The report by Indiana University Bloomington continues with numerous other
strategies and solutions, establishing a standard that previously did not
exist. 

Even well-funded preservation projects can take several years to complete. The
preservation management strategy that an institution has in place today can
give researchers a good indication of where they will be a decade from now. If
academic libraries do not have an actionable and solidified plan to digitize
and preserve magnetic media at the time of our research survey, they will
likely lose a significant portion of their magnetic media to degradation or
obsolescence. While Indiana University Bloomington’s preservation program is
more robust than any other institution in the United States, their preservation
management strategies can be applied in varying degrees to any institution.
These strategies will be considered best practices for the purpose of this
study.

Good preservation management strategies have been established in a few
institutions, but in order to implement them, archivists must be aware of the
stakes. By determining the amount of magnetic media in archives, the ratio of
the media that has been digitized, and the formats on hand, the true scope of
the problem can be assessed. Allowing archivists to better determine the time
and money needed to save these valuable collections, and to advocate for
greater resources with which to do so.

# Methods


This study involved mixed methodologies to answering the research question:
_What is the preservation status of magnetic media collections in academic
libraries and archives, and to what degree are they endangered by degradation
and obsolescence?_ From a questionnaire fulfilled by a sample of academic
libraries, quantitative data was gathered on collection size, format composition,
and digitization percentage to evaluate the current state of magnetic media
collections. Along with quantitative data, qualitative data was collected through
interviews on preservation management strategies to evaluate institutions’
responses to the magnetic media crisis.

The questionnaire was the primary data collection tool. Most, if not all,
academic institutions maintain records of the items within their collections,
and more often than not, they also track the formats of those items.
If an institution does not track this sort of data, it is an indication that
the institution does not have a comprehensive preservation management plan
prepared to properly address the magnetic media crisis. 

The first portion of the questionnaire asked for numerical values pertaining to
the amount and breakdown of magnetic media the institution holds as well as the
number of content hours that magnetic media contains. The second portion of the
questionnaire asked the institution about current preservation management
strategies in place, and how prepared they felt they were to deal with the
degradation and obsolescence of magnetic media.

The survey was created and hosted through the online platform SurveyMonkey and
disseminated through various electronic channels. These included SAA, ALA, and
AMIA listservs, Twitter, and emails sent to individual contacts. Email messages 
were adapted depending on the profession/personal relationships we had with the 
individual librarians and archivists. The questionnaire was initially circulated 
to all listservs on the night of November 7th and closed on November 21, 2018—a 
total of two full weeks. Throughout the two weeks in which the survey was open,
individual and institutional reference emails were messaged for continual
outreach.  Once the questionnaire closed, data was cleaned and processed using
OpenRefine and Google Sheets. R was used to create visualizations and
additional data analysis after initial findings warranted a deeper evaluation.

## Interviews 

Phone, video, and in-person interviews with various audiovisual archivists,
digital preservationists, and library managers—practitioners and
researchers—provided enhanced qualitative understanding of the preservation
management strategies for institutions dealing with magnetic media collections.
While the main focus was on the questionnaire for the quantitative effects,
approximately thirty-minute conversations with several professionals offered
more in-depth conversations that could not be gleaned from a short online
questionnaire. These interviews questions focused on topics such as:

- How do institutions prioritize their collections?
- How are institutions staffed and budgeted?
- What resources are allocated to support the preservation program?

Although conversations often departed and exceeded this script. In total, six
interviews were conducted—four with archivists at academic libraries, one with
an archivist at a state historical society, and one with an independent expert
on magnetic tape preservation.

Interview questions expanded on the obstacles archivists are facing in
preserving magnetic media and offered more qualitative data-points on
archivist’s experiences. These data-points include individual institutions’
successes and failures, challenges, actions institutions needs to make in order
to address magnetic media obsolescence and degradation, etc.

# Results

```{r load_libraries}
library(tidyverse)
library(ggrepel)
library(egg)
library(gplots)
library(RColorBrewer)
```

```{r read_data_risks,cache=T}
raw_datar <- read_csv("survey_data_2019.csv" ,
    col_names=c(
        "Key","State","InstitutionType","TotalItems",
        "StorageConditions",
        "Unstructured_StorageHistory",
        "Unstructured_CompletionEstimate",
        "PercentDigitizedBeforeUnplayable",
        "PracticeSelfAssesment",
        "Percentage_Open Reel Video","Percentage_U-Matic","Percentage_Betamax",
        "Percentage_VHS & S-VHS","Percentage_Betacam & Betacam SP",
        "Percentage_Video 8 & Hi8","Percentage_D2","Percentage_D3",
        "Percentage_DVCAM","Percentage_MiniDV","Percentage_DVCPro",
        "Percentage_Digital 8","Percentage_Open Reel Audio",
        "Percentage_Compact Cassette","Percentage_Microcassette",
        "Percentage_DAT","Percentage_DTRS","Percentage_F-1","Percentage_DCC",
        "Percentage_8-Track","Percentage_Other"),
    skip=1
    ) %>%
    mutate(PercentDigitizedBeforeUnplayable=sub("%","",
            PercentDigitizedBeforeUnplayable)
        )
all_responses <- raw_datar

risk_factors <- read_tsv("risk_assesment.tsv") %>%
    mutate(Risk=factor(Risk,
            levels=c("Low","Moderate","High","Very High","Extremely High")
        )   ) 

tdatar <- all_responses %>%
    gather(Format,Percentage,starts_with("Percentage_")) %>%
    mutate(Percentage=as.numeric(sub("%","",Percentage))) %>%
    mutate(EstItemCount=TotalItems*Percentage/100) %>%
    mutate(FormatName=sub("Percentage_","",Format)) %>% 
    left_join(risk_factors%>%rename(FormatName=Format),by="FormatName")
```

## Response Collections by Region and Climate

See chloreoplath files

## Respondents are mostly self-identified as academic

This study was targeted towards academic libraries as the main focus, 
but other archival institutions have enthusiastically participated in
our survey. 
We received ``r nrow(all_responses)`` responses, while 
``r sum(all_responses$InstitutionType!="Academic",na.rm=T)`` of these do not 
identify as "Academic".

```{r responses_by_instutition_type,cache=T}
all_responses %>%
    {
    ggplot(.)+theme_bw()+
    aes(x=InstitutionType)+
    geom_bar()+
    scale_x_discrete(limits=names(sort(table(.$InstitutionType),decreasing=T)))+
    geom_label(aes(label=..count..,y=..count..),stat="count")+
    xlab("Institution type")+
    ylab("Responses")
    }
```

## Most but not all facilities store media in controlled conditions

We asked respondents to categorize their storage conditions for their 
collections. 
The different environmental archival levels are based on the Video
Preservation Fact Sheet from AMIA. 
While the majority of the participants maintain 
“temperature/humidity controlled” storage, some respondents report 
“actively harmful” and “uncomfortable for humans” environments.
``r sum(is.na(all_responses$StorageConditions))`` respondents 
didn't include this information.

Many archivists have access to at least a temperature/humidity controlled
storage facility and are expecting have most of their magnetic media collection
successfully digitized. However, some report poor storage conditions or a bleak
outlook for their collection. Support and coordination amongst digital media
archivists may help these archivists plan and advocate for their collection.

```{r responses_by_storage_conditions,cache=T,fig.height=7}
all_responses %>%
    filter(!is.na(StorageConditions)) %>%
    {
    ggplot(.)+theme_bw()+
    aes(x=StorageConditions)+
    geom_bar()+
    scale_x_discrete(limits=c(
            "Archival (vacuum sealed items, cold storage, etc.)",
            "Temperature/Humidity controlled",
            "Room temperature",
            "Uncomfortable for human",
            "Actively harmful (dirty, hot, humid, etc.)"
            )
        )+
    geom_label(aes(label=..count..,y=..count..),stat="count")+
    theme(axis.text.x=element_text(angle=90))+
    xlab("")+
    ylab("Responses")
    }
```

## Archivist skill self-evaluation ranges widely

Respondents were asked to self-evaluate their skills

<!-- yall should actually put the question here -->

We expected to find that most archivists were above average, on average, but
believe that the distribution of self-assessment reflects a healthy 
self-criticism in the field. 

Future work with surveys or interviews may find it useful to probe where 
archivists feel their skills are falling short.

```{r responses_by_self_reflection,cache=T,fig.height=7}
all_responses %>%
    filter(!is.na(PracticeSelfAssesment)) %>%
    {
    ggplot(.)+theme_bw()+
    aes(x=PracticeSelfAssesment)+
    geom_bar()+
    scale_x_discrete(limits=c(
            "Excellent",
            "Above Average",
            "Average",
            "Below Average",
            "Poor"
            )
        )+
    geom_label(aes(label=..count..,y=..count..),stat="count")+
    theme(axis.text.x=element_text(angle=90))+
    xlab("")+
    ylab("Responses")
    }
```

## Many items are estimated to decay before digitization

Of the ``r sum(!is.na(all_responses$PercentDigitizedBeforeUnplayable))``
respondents who estimated how much magnetic material
they would be able to digitize, eight—nearly a quarter—were confident they
would process every item in their collection. However, almost as many
institutions expressed doubt that they would digitize even a quarter of their
collections before decay made their tapes unplayable. From a cursory
examination, there does not appear to be a strict correlation between the size
of the collection and the estimated digitization rate. 

what guidelines were supplied to help archivists make this assessment?

Table 2: Respondent
Institutions’ Digitization Progress Rates lists institutions’ current progress
rate, calculated by their yearly digitization rate as a percentage of their
total collection.

The bimodality may reflect a binary choice for the respondent, but it is 
striking that a significant fraction find themselves with little hope for
preserving their collection.

```{r percent_completion,cache=T}
all_responses %>%
    filter(!is.na(PercentDigitizedBeforeUnplayable)) %>%
    {
    ggplot(.)+theme_bw()+
    aes(x=as.numeric(PercentDigitizedBeforeUnplayable))+
    stat_bin(binwidth=05,geom="bar",col="white")+
    xlab("Self-assessed estimate of complete digitization\nbefore media is unplayable (binned into tenths)")+
    ylab("Responses per bin (width")
    }
```

```{r storage_conditions_and_completion,cache=T,fig.height=7,eval=F}
g_storage_vs_complete <- all_responses %>%
    filter(!is.na(StorageConditions)) %>%
    filter(!is.na(PercentDigitizedBeforeUnplayable)) %>%
    {
    ggplot(.)+
    theme_article()+scale_fill_distiller(palette="Spectral")+
    aes(x=as.numeric(PercentDigitizedBeforeUnplayable),y=StorageConditions)+
    scale_y_discrete(limits=c(
            "Archival (vacuum sealed items, cold storage, etc.)",
            "Temperature/Humidity controlled",
            "Room temperature",
            "Uncomfortable for human",
            "Actively harmful (dirty, hot, humid, etc.)"
            )
        )+
    stat_bin2d(binwidth=c(10,1))+
    xlab("Self-assessed estimate of complete digitization\nbefore media is unplayable")+
    ylab("Responses per bin")+
    scale_x_continuous(breaks=seq(0,100,10))
    }
g_storage_vs_complete
```


```{r making_figures_for_poster,cache=T,fig.height=7}
g_storage <- all_responses %>%
    filter(!is.na(StorageConditions)) %>%
    {
    ggplot(.)+theme_article()+
    aes(x=StorageConditions)+
    scale_x_discrete(limits=c(
            "Archival (vacuum sealed items, cold storage, etc.)",
            "Temperature/Humidity controlled",
            "Room temperature",
            "Uncomfortable for human",
            "Actively harmful (dirty, hot, humid, etc.)"
            ),
            labels=c(
            "Archival\n(vacuum sealed items,\ncold storage, etc.)",
            "Temperature/Humidity\ncontrolled",
            "Room temperature",
            "Uncomfortable\nfor humans",
            "Actively harmful\n(dirty, hot, humid, etc.)"
            )
        )+
    geom_bar(fill="grey40",color="black")+
    #geom_label(aes(label=..count..,y=..count..),stat="count")+
    scale_y_continuous(breaks=seq(0,40,5))+
    theme(axis.text.x=element_text(angle=90))+
    xlab("")+
    ylab("Responses")
    }
g_complete <- all_responses %>%
    filter(!is.na(PercentDigitizedBeforeUnplayable)) %>%
    {
    ggplot(.)+theme_article()+
    aes(x=as.numeric(PercentDigitizedBeforeUnplayable))+
    stat_bin(binwidth=10,center=5,geom="bar",fill="grey40",color="black")+
    xlab("Self-assessed estimate of complete\ndigitization before media is unplayable")+
    ylab("Responses")+
    scale_x_continuous(breaks=seq(0,100,10))+
    scale_y_continuous(breaks=seq(0,10,1))
    }
g_both <- ggarrange(g_storage,g_complete,nrow=1,labels=c("A","B"),draw=F)
ggsave(paste0(fig_dir,"storageConditions_and_completionEstimate.svg"),
    g_both,width=6,height=4)
ggsave(paste0(fig_dir,"storageConditions_and_completionEstimate.png"),
    g_both,width=6,height=4)
```

## There are not as many items left in formats at high-risk of decay

Using the classifications from 

To prioritize digitization planning, we compared the sume total of items per
format to each format's obsolescence risk factor (CLIR, 1995). We found a
relatively low number of high-risk formats, suggesting that niche high-risk
formats may be preservable with focused efforts. It is worth noting that this
pattern may reflect a survivor's bias in collections. 

Comparing the format's risk of decay with the estimated total item count
allows us to prioritize what formats to target immediately with focused grants.

```{r format_counts_divided_x_by_risk,cache=T,fig.height=7}
g <- tdatar %>% 
    group_by(FormatName,Risk) %>%
    summarize(EstTotal=sum(EstItemCount,na.rm=T)) %>%
    filter(!is.na(Risk)) %>%
    filter(EstTotal>0) %>%
    ggplot()+
    theme_bw()+
    aes(x=Risk,y=EstTotal,label=FormatName)+
    geom_text_repel(box.padding=1,segment.color="grey60")+
    theme(axis.text.x=element_text(angle=90))+
    ylab("Total estimated items, across survey")+
    xlab("Risk Estimate")

g_risk_totals <- g+
    scale_y_continuous(labels=function(x){formatC(x,big.mark=",",format="d")},
        breaks=seq(0,2e5,2e4),limits=c(-2e4,NA))+
        geom_dotplot(binaxis="y",stackdir="center",binwidth=3e3) 
g_risk_totals
```

```{r saving_risk_totals_figure,cache=T}
ggsave(paste0(fig_dir,"risk_and_totals.png"),
    g_risk_totals,width=6,height=6)
ggsave(paste0(fig_dir,"risk_and_totals.svg"),
    g_risk_totals,width=6,height=6)
```

## Format abundances vary widely between institutions

Nearly
all participating institutions have at least one VHS tape, but there are only 3
archives that need to deal with Digital 8. Either item counts or percentages of
collection were very heterogeneous, suggesting that there is not one shared set
of formats that would be served by a standard set of equipment. Would a
centralized digitization center best address rare formats?

```{r total_numbers_of_items_each_format_dist,cache=T}
g <- tdatar %>% 
    {
    ggplot(.)+
    theme_bw()+
    aes(x=FormatName,y=EstItemCount)+
    geom_boxplot(outlier.alpha=0,color="grey50")+
    geom_dotplot(binaxis="y",stackdir="center",dotsize=1.0,alpha=1.0,binwidth=0.05)+
    scale_y_log10(labels=function(x){formatC(x,big.mark=",",format="d")})+
    scale_x_discrete(
        limits=group_by(.,FormatName)%>%
            summarize(z=sum(EstItemCount,na.rm=T))%>%
            arrange(-z)%>%pull(FormatName)
        )+
    theme(axis.text.x=element_text(angle=90))+
    ylab("Each respondent's estimated items per format")+
    xlab("")
    }
g
ggsave(paste0(fig_dir,"items_per_respondent.png"),g,width=6,height=5)
ggsave(paste0(fig_dir,"items_per_respondent.svg"),g,width=6,height=5)
```

A similar pattern is seen in examining the proportion of each collection that is
a format.

```{r proportion_of_items_each_format_dist,cache=T}
g <- tdatar %>% 
    {
    ggplot(.)+
    theme_bw()+
    aes(x=FormatName,y=Percentage)+
    geom_boxplot(outlier.alpha=0,color="grey50")+
    geom_dotplot(binaxis="y",stackdir="center",alpha=1.0,binwidth=0.02)+
    scale_y_log10(labels=function(x){formatC(x,big.mark=",",format="d")},
        limits=c(0.9,NA))+
    scale_x_discrete(
        limits=group_by(.,FormatName)%>%
            summarize(z=sum(EstItemCount,na.rm=T))%>%
            arrange(-z)%>%pull(FormatName)
        )+
    theme(axis.text.x=element_text(angle=90))+
    ylab("Each respondent's estimated items per format")+
    xlab("")
    }
g
```

## Overall perspective on survey results

This heat map illustrates an aggregation of the above charts and graphs. The
hierarchical clustering shows how each format is related or unrelated
considering the factors of obsolesence risk and quantity held within
participating archives' collections. At the top of the chart, there are two
major goups: (1) VHS, open reel audio, and compact cassettes, and (2) all other
formats. This distinction is from the quantity that instutions hold, in that
group 1 is more likely to have a higher quantity than group 2. From quantity
and risk factor, there is a noticeable relationship with Betacam and u-matic.
This suggests that if an archive carries of one of these format types, they are
likely to also have the latter in their collection. This further leads to the
idea that training or programming for Betacam and U-matic together could be an
efficient approach to dealing with those collections.


```{r 2nd_heatmap_counts,fig.height=9,fig.width=9,cache=T}
heatmap_datar_counts <- tdatar %>% 
    filter(EstItemCount>0) %>%
    select(Key,State,InstitutionType,TotalItems,
        PercentDigitizedBeforeUnplayable,FormatName,EstItemCount) %>%
    spread(FormatName,EstItemCount,fill=0) %>%
    arrange(-as.numeric(PercentDigitizedBeforeUnplayable))
heatmap_datar_counts%>%pull(PercentDigitizedBeforeUnplayable)

counts_matrix <- as.matrix(log10(heatmap_datar_counts[,-c(1,2,3,4,5)]+1))

# Thanks colorbrewer2 website!
colorz <- setNames(c('#ffffb2','#fecc5c','#fd8d3c','#f03b20','#bd0026'),
        levels(risk_factors$Risk))
risk_palette <- colorz[
        tibble(Format=colnames(heatmap_datar_counts)[
                6:ncol(heatmap_datar_counts)]
            )%>%    
            left_join(risk_factors,by="Format")%>%pull(Risk)
        ] 
k <- data.frame(fill=colorz[levels(risk_factors$Risk)],y=levels(risk_factors$Risk)) %>%
    ggplot()+theme_classic()+
    aes(fill=y,x=1,y=y)+
    scale_fill_manual("Risk",values=colorz)+
    scale_y_discrete(limits=levels(risk_factors$Risk))+
    guides(fill=F)+
    geom_tile()+
    theme(axis.text.x=element_blank(),axis.ticks=element_blank(),
        axis.line=element_blank())+
    ylab("")+xlab("")
k
ggsave(paste0(fig_dir,"heatmap_key.png"),k,width=3,height=2)

tickfunc <- function() {
            breaks <- 10^seq(1,5,0.1)
            which_ones <- c(11,21,31,41)
            return(list(
                at=parent.frame()$scale01(log10(breaks[which_ones])),
                labels=formatC(breaks[which_ones],big.mark=",",format="d")
                ))
           }


heatmap.2(counts_matrix,
    key.xlab="Estimated Items",
    col=colorRampPalette(brewer.pal(n=9,name="PuBuGn"))(100),
    ColSideColors=risk_palette,
    margins=c(08,5),
    labRow=ifelse(!is.na(heatmap_datar_counts$PercentDigitizedBeforeUnplayable),
        paste0(heatmap_datar_counts$PercentDigitizedBeforeUnplayable,"%"),"no data"),
    srtCol=45,
    density.info="none",
    trace="none",
    dendrogram="col",
    Rowv=F,
    reorderfun=function(d, w) rev(reorder(d, w, agglo.FUN = mean)),
    ylab="Respondent, ordered by estimated completion",
    key.xtickfun=tickfunc
    )
#svg("magnetic_heatmap_ordered_by_chance_completion_at_top.svg",width=6,height=6)
# dev.off()

#svg("magnetic_heatmap_clustered_rows.svg",width=6,height=6)
heatmap.2(counts_matrix,
    key.xlab="Estimated Items",
    col=colorRampPalette(brewer.pal(n=9,name="PuBuGn"))(100),
    ColSideColors=risk_palette,
    margins=c(08,5),
    labRow=ifelse(!is.na(heatmap_datar_counts$PercentDigitizedBeforeUnplayable),
        paste0(heatmap_datar_counts$PercentDigitizedBeforeUnplayable,"%"),"no data"),
    srtCol=45,
    density.info="none",
    trace="none",
    dendrogram="both",
    reorderfun=function(d, w) rev(reorder(d, w, agglo.FUN = mean)),
    ylab="Respondent",
    key.xtickfun=tickfunc
    )
#dev.off()

```

Clustering of the format's similarities shows that VHS, Compact Cassette, and
Open Reel Audio tend to have similar patterns of abundance across collections.
U-Matic, Betacam, Open Reel Video, and MiniDV show a similar cluster of
co-variability. Each row denotes an institution's response, and is sorted by
the estimated chance of digitization of their media before deterioration
occurs. 



correlation matrix

# Conclusions

While the data comes from a limited pool of academic archives and therefore cannot speak for all archives, the data can still serve to indicate what may be expected. Additional research addressing the preservation status of magnetic media collections in academic libraries, and to what degree are they endangered, could only make a stronger case for funding proposals. However, even without further research, it is imperative to report that archivists must begin by conducting assessments and evaluations of their collections in order to allocate resources to digitize their magnetic media collections. This is a crucial first step that needs to happen now, not later during the institution’s five-year plan. Moreover, this study illustrates that many institutions have not taken this first step.

# Future Opportunities

# References

# Acknowledgements

Many thanks to: our LIS 570 collaborators, Brian Click and Michael Kuster, who helped with the literature review, method design, and co-writing the original paper. Paul J. Weiss, LIS 570 instructor, who inspired and guided us to use this project to engage and contribute to the larger archives community. All survey respondents, interviewees, and archive listervs who welcomed our research to the discussion forums. Thank you SAA for accepted our research to the graduate student poster presentation. 

# Supplemental Analyses

Because how often do you get to see the axes of variation amongst human 
institutions? Be they collections or formats both are dope !

```{r,cache=T}
pca_datar <- tdatar %>% select(Key,TotalItems,FormatName,EstItemCount) %>%
    spread(FormatName,EstItemCount,fill=0)

matrix_by_inst <- as.matrix(pca_datar[,-c(1,2)])
rownames(matrix_by_inst) <- pca_datar[,1][[1]]
cluster_inst <- hclust(dist(scale(matrix_by_inst))) 
matrix_by_media <- t(as.matrix(pca_datar[,-c(1,2)]))
cluster_media <- hclust(dist(scale(matrix_by_media)))
```

## PCA of media format

What major patterns of variation amongst formats are seen, with respect to their
inventory in different institutions?
The ways in which some factors are similar to each other and then distinct to others.Groupings of how formats are distributed differently. 
 

```{r pca_media,cache=T}
pca_media <- prcomp(t(matrix_by_media[apply(matrix_by_media,1,sum)>0,]),center=T,scale=T)

media_pca_plot <- pca_media$rotation %>% 
    {bind_cols(tibble(Format=rownames(.)),as.tibble(.))} %>%
    select(Format,PC1,PC2,PC3,PC4,PC5,PC6) %>% 
    ggplot()+theme_bw()+
    aes(label=Format)+
    geom_point()+
    geom_label_repel()
media_pca_plot + aes(x=PC1,y=PC2)
```

This is where you say there's a big main trend of the things in the top right
are a group, things more to the left are not, this is probably generally 
popularity.

Stuff on bottom is the first distinct cluster.

```{r pca_media2,cache=T}
media_pca_plot + aes(x=PC3,y=PC4)
```

Betacam and Other are special, MiniDV in it's own special place


## PCA of institutions

What major patterns of variation amongst formats are seen, with respect to their
inventory in different institutions?

For completeness, these are all plots.

Note plot 2 (3 vs 4) shows how museums and historical societies are way 
different, probably have a lot of Other formats.

More surveys needed

```{r pca_inst,cache=T}
pca_inst <- prcomp(t(matrix_by_inst[apply(matrix_by_inst,1,sum)>0,] ),center=T,scale=T)
percent_varz <- signif(pca_inst$sdev / sum(pca_inst$sdev) * 100,3)

filtered_mbi <- matrix_by_inst[apply(matrix_by_inst,1,sum)>0,]

pdatar <- pca_inst$rotation %>% 
    {bind_cols(tibble(Key=rownames(.)),as.tibble(.))} %>%
    select(Key,PC1,PC2,PC3,PC4,PC5,PC6) %>% 
    left_join(all_responses%>%mutate(Key=as.character(Key)),by="Key") 

inst_pca_plot <- pdatar %>%
    ggplot()+theme_bw()+
    aes(label=Key)+
    geom_point()+
    geom_label_repel()
pc12 <- inst_pca_plot+aes(x=PC1,y=PC2)+
    xlab(paste0("PC1 ",percent_varz[1],"% variance"))+
    ylab(paste0("PC2 ",percent_varz[2],"% variance"))
pc34 <- inst_pca_plot + aes(x=PC3,y=PC4)+
    xlab(paste0("PC3 ",percent_varz[3],"% variance"))+
    ylab(paste0("PC4 ",percent_varz[4],"% variance"))
pc56 <- inst_pca_plot + aes(x=PC5,y=PC6)+
    xlab(paste0("PC5 ",percent_varz[5],"% variance"))+
    ylab(paste0("PC6 ",percent_varz[6],"% variance"))

pc12+aes(fill=InstitutionType)
pc34+aes(fill=InstitutionType)
pc56+aes(fill=InstitutionType)
```

Huh what's on each of these axis? Why are museums different than academic 
archives?

Here's a plot of the correlation of each format with each axis.
Labelled are one-test significant for pearson correlation.

```{r pca_dig_in_pearson,cache=T}
z <- apply(pca_inst$rotation[,1:6],2,
    function(x){
       lapply(apply(filtered_mbi,2,cor.test,x,method="pearson"),
            function(y){
                data.frame(pval=y$p.value,est=y$estimate)
            })
    })
rez <- data.frame(PC=NA,Format=NA,pval=NA,est=NA)
for (i in names(z)) {
    for (j in names(z[i][[1]])) {
        rez <- rbind(rez,cbind(data.frame(PC=i,Format=j),z[i][[1]][j][[1]]))
    }
}
rez <- rez[-1,]
# god that's ugly, I forgot how to do a for loop for a bit there
ggplot(rez)+theme_bw()+
    aes(label=Format,x=-log10(pval),y=est)+
    facet_wrap(~PC)+
    geom_point()+
    geom_label_repel(data=subset(rez,pval<0.05))
```


And if we replot some of those back on the PCA plots:
Bridge the gap between institution types. Consider GLAM practices but also this shows the instittution type does hold different assets, which means different needs. 

```{r replot_pcas,cache=T}
pc14 <- inst_pca_plot+aes(x=PC1,y=PC4,fill=as.numeric(sub("%","",Percentage_Other)))+
    xlab(paste0("PC1 ",percent_varz[1],"% variance"))+
    ylab(paste0("PC4 ",percent_varz[4],"% variance"))+
    scale_fill_distiller("Other",palette="Spectral",na.value="lightblue")
pc14

pc23 <- inst_pca_plot + aes(x=PC2,y=PC3)+
    xlab(paste0("PC2 ",percent_varz[2],"% variance"))+
    ylab(paste0("PC3 ",percent_varz[3],"% variance"))

pc23+aes(fill=as.numeric(sub("%","",`Percentage_Other`)))+
    scale_fill_distiller("Other",palette="Spectral",na.value="lightblue")

pc23+aes(fill=0+
        as.numeric(sub("%","",`Percentage_U-Matic`))+
        as.numeric(sub("%","",`Percentage_Betacam & Betacam SP`))
        )+
    scale_fill_distiller("Umatic or Betacam",palette="Spectral",na.value="lightblue")

```

And for spearman
```{r pca_dig_in_spearman,cache=T}
z <- apply(pca_inst$rotation[,1:6],2,
    function(x){
       lapply(apply(filtered_mbi,2,cor.test,x,method="spearman"),
            function(y){
                data.frame(pval=y$p.value,est=y$estimate)
            })
    })
rez <- data.frame(PC=NA,Format=NA,pval=NA,est=NA)
for (i in names(z)) {
    for (j in names(z[i][[1]])) {
        rez <- rbind(rez,cbind(data.frame(PC=i,Format=j),z[i][[1]][j][[1]]))
    }
}
rez <- rez[-1,]
# god that's ugly, I forgot how to do a for loop for a bit there
ggplot(rez)+theme_bw()+
    aes(label=Format,x=-log10(pval),y=est)+
    facet_wrap(~PC)+
    geom_point()+
    geom_label_repel(data=subset(rez,pval<0.05))
```


```{r reigon,cache=T,echo=F,eval=F}
pc12+aes(fill=c(
    `Wisconsin`="Central",
    `Washington D.C.`="East",
    `Massachusetts`="East",
    `New York`="East",
    `California`="West",
    `Iowa`="Central",
    `Hawaii`="Out",
    `Pennsylvania`="East",
    `Oregon`="West",
    `Colorado`="West",
    `Delaware`="East",
    `Idaho`="West",
    `Indiana`="Central",
    `Maryland`="East",
    `Washington`="West",
    `Louisiana`="South",
    `New Mexico`="West",
    `Ohio`="Central",
    `British Columbia`="West",
    `Texas`="South",
    `Alaska`="Out",
    `Florida`="South",
    `Missouri`="South",
    `Nebraska`="Central",
    `Utah`="West",
    `Virginia`="East",
    `Wyoming`="West",
    `Kentucky`="East"
    )[State])
```



```{r pheatmap,echo=F,eval=F}
##library(pheatmap)
##row_ann <- data.frame(
##    Collection=str_c("C_",as.character(rownames(matrix_by_inst)[cluster_inst$order]))
##    )
##rownames(row_ann) <- as.character(row_ann[[1]])
##
##col_ann <- data.frame(Format=rownames(matrix_by_media)[cluster_media$order]) %>%
##    left_join(totals_and_risks,by="Format") %>% select(-variable) %>%
##    rename(`Total Items`=Total,`Format Risk`=Risk) %>%
##    select(-`Total Items`)
##rownames(col_ann) <- col_ann[[1]]
##col_ann <- col_ann %>% select(-Format)
##count_palette <- c("#FFFFFF",
##    colorRampPalette((RColorBrewer::brewer.pal(n=7,name="YlGnBu")))(99)
##    )
##
##g <- pheatmap( log10(heatmap_datar[,-c(1,2)]+400),
##    color=count_palette,
##    correlation_distance_rows="pearson",
##    correlation_distance_cols="pearson",
##    legend_breaks=log10(c(10,100,1000,3000,1e4,2.5e4,5e4)),
##    legend_labels=c(10,100,1000,3000,1e4,2.5e4,5e4),
##    cluster_rows=cluster_inst,
##    cluster_cols=cluster_media,
##    show_rownames=F,show_colnames=T,
##    #annotation_row=row_ann,
##    annotation_col=col_ann,
##    annotation_legend=T)
##g
```
